{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 強化学習で実装したいもの\n",
    "- 方策\n",
    "    - π(a = a1|s1) = 0.3\n",
    "        - 状態s1の時にa1を行う確率は0.3\n",
    "        - 最終的にはこの方策が1つに決まる。(つまり100％これと言えるものを得る)\n",
    "                - それがゴール\n",
    "- 状態遷移確率\n",
    "    - p(s = s2|s1,a = a1) = 0.6\n",
    "        - 主にs = s1の時に方策の確率に従いa1の行動を実行した時にs2に映る確率を意味する。\n",
    "- 報酬\n",
    "    - r(s1,a,s1)\n",
    "        - 主にs1からaの行動を行い,s2に移動した時に発生する報酬\n",
    "- これらを用いて導き出せる漸化式がベルマン方程式\n",
    "    - つまりベルマン方程式を解けば、方策が1つにきまり、最適な解が見つかる。(決定論的な方策\n",
    "## 最終的に\n",
    "- 方策が1意に決まり、それに基づき状態価値関数も1位に決まる。\n",
    "- 今回は状態遷移確率は1とする(つまり行動が決まれば必ずその行動通りに動く)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 環境の実装\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        #行動は4種類(それぞれを数字に置き換える)\n",
    "        self.action_list = [0,1,2,3]\n",
    "        # 辞書型で行動を紐づける\n",
    "        self.action_meaning = {\n",
    "            0: \"UP\",\n",
    "            1: \"DOWN\",\n",
    "            2: \"LEFT\",\n",
    "            3: \"RIGHT\"\n",
    "        }\n",
    "        # 以下にマップを作成する\n",
    "        # それぞれの値はその場所の報酬を指す\n",
    "        self.reward_map = np.array(\n",
    "            [[0,0,0,1],\n",
    "            [0,None,0,-1],\n",
    "            [0,0,0,0]]\n",
    "        )\n",
    "        #以下に障害物を記載し、その座標を保持\n",
    "        self.goal = (0,3)\n",
    "        self.start = (2,0)\n",
    "        self.wall = (1,1)\n",
    "        #エージェントの場所の配列を保持(スタート値で初期化)\n",
    "        self.agent_state = self.start\n",
    "\n",
    "    # 以下にメソッドを実装\n",
    "    # マップの情報を取得(主に再代入不可能のgetterの役割)\n",
    "    # 高さを返却\n",
    "    @property\n",
    "    def height(self):\n",
    "        return len(self.reward_map)\n",
    "    # 横幅を返却\n",
    "    @property\n",
    "    def width(self):\n",
    "        return len(self.reward_map[0])\n",
    "\n",
    "    #行動の種類を返却\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self.action_list\n",
    "\n",
    "    def states(self):\n",
    "        for h in range(self.height):\n",
    "            for w in range(self.width):\n",
    "                yield (h,w)\n",
    "\n",
    "\n",
    "    # 以下にエージェントを動かすための関数と報酬関数を実装する。\n",
    "    def next_step(self,state,action):\n",
    "        # 上下左右の行動をそれぞれ座標に足し合わせることで実装\n",
    "        action_list = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "        # 0~3までの数字を配列に入れることで行動となる数値を取得\n",
    "        move = action_list[action]\n",
    "        # エージェントを移動させる(moveを現在の状態の座標に足し合わせる)\n",
    "        # next_state = (y座標,x座標)となる\n",
    "        next_state = (state[0] + move[0],state[1] + move[1])\n",
    "        new_y,new_x = next_state\n",
    "\n",
    "        # 移動したのちそれがマップから外れてるかどうかの確認処理(外れていたら更新を中断)\n",
    "        if new_x < 0 or new_x >= self.width or new_y < 0 or new_y >= self.height or next_state == self.wall:\n",
    "            next_state = state\n",
    "        return next_state #(2,3)みたいに次の座標\n",
    "    # 報酬関数\n",
    "    def reward(self,state,action,next_state):\n",
    "        return self.reward_map[next_state] # 1みたいに移動先での報酬を受け取る"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(2, 0)\n",
      "(2, 1)\n",
      "(2, 2)\n",
      "(2, 3)\n",
      "{(0, 0): 0, (0, 1): 0, (0, 2): 0, (0, 3): 0, (1, 0): 0, (1, 1): 0, (1, 2): 0, (1, 3): 0, (2, 0): 0, (2, 1): 0, (2, 2): 0, (2, 3): 0}\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "print(env.actions)\n",
    "V = {}\n",
    "for state in env.states():\n",
    "    V[state] = 0\n",
    "    print(state)\n",
    "print(V)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 動的計画法の実装\n",
    "## 必要なこと\n",
    "1. まずは状態価値関数を出力しなくてはいけない\n",
    "2. それが決まることで状態行動価値関数を次に実装することができる\n",
    "    - その状態行動価値関数を実装するために必要なのが反復方策評価\n",
    "\n",
    "## 以下に動的計画法を行い状態価値関数を導きだす\n",
    "- 方策はどのマスも一定で上下左右に1/4の確率で移動するため、これを辞書型配列で保持し、確率と行動の番号を紐づける\n",
    "-  動的計画法を用いて更新を続けていく\n",
    "- そのためには辞書型配列をまずは初期化する\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (0, 1): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (0, 2): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (0, 3): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (1, 0): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (1, 1): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (1, 2): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (1, 3): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (2, 0): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (2, 1): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (2, 2): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}, (2, 3): {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}}\n",
      "[(0, 0), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(0, 1), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(0, 2), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(0, 3), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(1, 0), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(1, 1), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(1, 2), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(1, 3), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(2, 0), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(2, 1), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(2, 2), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n",
      "[(2, 3), {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}]\n"
     ]
    }
   ],
   "source": [
    "env2 = GridWorld()\n",
    "V = {}\n",
    "for state in env2.states():#(0,0),(0,1)...\n",
    "    V[state] = {0:0.25,1:0.25,2:0.25,3:0.25}\n",
    "print(V)\n",
    "for i,j in V.items():\n",
    "    print([i,j])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 以下に動的計画法の実装関数を書く\n",
    "def eval_step(pi,V,env,gamma=0.9):\n",
    "    # stateで全ての座標にアクセスして価値関数を更新していく(最後までエピソードが終わった時の報酬がその場所の価値関数になる)\n",
    "    for state in env.states():#(0,0),(0,1)...\n",
    "        # アクセスした座標がゴールだった場合は価値関数は0なので0にする\n",
    "        if state == env.goal:\n",
    "            V[state] = 0\n",
    "            continue\n",
    "        action_probs = pi[state] #{0:0.25,1:0.25,2:0.25,3:0.25}\n",
    "        new_V = 0\n",
    "\n",
    "        # 各座標と方策を取得\n",
    "        for action,action_prob in action_probs.items():# action=0 action_prob=0.25\n",
    "            next_state = env.next_step(state,action)\n",
    "            r = env.reward(state,action,next_state)\n",
    "            # 価値関数を更新していく(動的計画法)\n",
    "            new_V += action_prob * (r+gamma*V[next_state]) # <-これがベルマン方程式\n",
    "        V[state] = new_V\n",
    "    return V\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 以上の関数を繰り返し行うことで(更新する数値がある値よりも少なくなった場合)状態価値関数が決定する\n",
    "def policy_eval(pi,V,env,gamma,check_value = 0.001):\n",
    "    while True:\n",
    "        # まずは以前使った状態価値関数を保持する(更新後と更新前で比較)\n",
    "        # また参照わたしにならないように値渡しを用いて(copy)保持\n",
    "        old_V = copy.copy(V)\n",
    "        V = eval_step(pi,V,env,gamma)\n",
    "\n",
    "        # 更新された量の最大値を求める\n",
    "        update_value = 0\n",
    "        for state in V.keys(): #state=(0,0),(0,1)...\n",
    "            t = abs(V[state] - old_V[state]) # 更新した値の絶対値を保持\n",
    "            update_value += t\n",
    "            # 更新された値がcheck_valueより小さかったらループを抜ける\n",
    "        if update_value < check_value:\n",
    "            break\n",
    "        # if update_value < check_value:\n",
    "        #     break\n",
    "    return V\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fcfc9749d80>, {(0, 0): 0.026346148835571733, (1, 0): -0.031114424981338266, (0, 1): 0.09502345507038304, (0, 2): 0.20570332045739587, (1, 2): -0.4976988125278732, (0, 3): 0, (2, 0): -0.10280186797327864, (2, 1): -0.22052705113027188, (1, 1): -0.14718761705988997, (2, 2): -0.43652592712473914, (1, 3): -0.37251078722377295, (2, 3): -0.7854756526392008})\n"
     ]
    }
   ],
   "source": [
    "# 実際に状態価値関数を実装する\n",
    "env = GridWorld()\n",
    "gamma = 0.9 #割引率\n",
    "pi = defaultdict(lambda: {0:0.25,1:0.25,2:0.25,3:0.25})\n",
    "V = defaultdict(lambda :0)\n",
    "V = policy_eval(pi,V,env,gamma)\n",
    "print(V)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02634615  0.09502346  0.20570332  0.        ]\n",
      " [-0.03111442 -0.14718762 -0.49769881 -0.37251079]\n",
      " [-0.10280187 -0.22052705 -0.43652593 -0.78547565]]\n"
     ]
    }
   ],
   "source": [
    "# ちょっとだけ綺麗に出力\n",
    "X = np.zeros((3, 4))\n",
    "for i,j in V.items():\n",
    "    a,b = i\n",
    "    X[a][b] = j\n",
    "print(np.array(X))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 方策反復法\n",
    "- 以上で各々の状態価値関数を出力することができたので、続いて方策反復法を用いて最適方策を求めていく\n",
    "- 以前に導出した状態価値関数をもとに、方策を更新していき、その方策の更新で状態価値関数が更新されということを繰り返していく\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object GridWorld.states at 0x7fcfc97033e0>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 方策を常に最大値に更新し続ける関数を作成する。\n",
    "def argmax(d):\n",
    "    max_value = max(d.values())\n",
    "    max_key = 0\n",
    "    for key,value in d.items():\n",
    "        if value == max_value:\n",
    "            max_key = key\n",
    "    # 行動の番号を返す(つまり前後左右どの方向に動くのがいいのかの番号を返す)\n",
    "    return max_key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 方策反復法を用いて実装を行う\n",
    "# 引数に状態価値関数,方策,割引率を受け取る\n",
    "def greedy_policy(V,env,gamma):\n",
    "    pi = {}\n",
    "    for state in env.states():# (0, 0),(0, 1),(0, 2),(0, 3),(1, 0)...\n",
    "        action_values = {}\n",
    "        for action in env.actions:# [0, 1, 2, 3]\n",
    "            next_state = env.next_step(state,action)\n",
    "            r = env.reward(state,action,next_state)\n",
    "            value = r + gamma * V[next_state]\n",
    "            action_values[action] = value\n",
    "        max_action = argmax(action_values)\n",
    "        action_probs = {0:0,1:0,2:0,3:0}\n",
    "        action_probs[max_action] = 1.0\n",
    "        pi[state] = action_probs\n",
    "    return pi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def policy_iter(env,gamma,check_value = 0.001):\n",
    "    pi = defaultdict(lambda :{0:0.25,1:0.25,2:0.25,3:0.25})\n",
    "    V = defaultdict(lambda :0)\n",
    "\n",
    "    while True:\n",
    "        V = policy_eval(pi,V,env,gamma,check_value)\n",
    "        new_pi = greedy_policy(V,env,gamma)\n",
    "\n",
    "        if new_pi == pi:\n",
    "            break\n",
    "        pi = new_pi\n",
    "    return pi\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "gamma = 0.9\n",
    "pi = policy_iter(env,gamma)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 出力結果をわかりやすく出力\n",
    "# [0,1,2,3]が[\"上\",\"下\",\"左\",\"右\"]に対応,また障害物,ゴールは100で表す。\n",
    "def ans(pi,env):\n",
    "    X = np.zeros((3, 4))\n",
    "    for i,j in pi.items():\n",
    "        a,b = i\n",
    "        if i == env.goal or i == env.wall:\n",
    "            X[a][b] = None\n",
    "        else:\n",
    "            move = argmax(j)\n",
    "            X[a][b] = move\n",
    "    return X\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 3.,  3.,  3., nan],\n       [ 0., nan,  0.,  0.],\n       [ 3.,  3.,  0.,  2.]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans(pi,env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}