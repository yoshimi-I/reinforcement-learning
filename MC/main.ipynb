{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# モンテカルロ法\n",
    "- 前回は動的計画法を用いて状態価値関数を実装したのち,方策反復法によって最適方策を得ることができた。\n",
    "    - しかし動的計画法では計算量が膨大になってしまう恐れがある、また状態遷移確率と報酬が既知である必要がある。\n",
    "        - 前回実装した環境では右に行こうとすれば必ず右に行くようになっていたが、そうはならない問題も存在する。\n",
    "    - そのための代替案の一つがモンテカルロ法である。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 環境の実装\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        #行動は4種類(それぞれを数字に置き換える)\n",
    "        self.action_list = [0,1,2,3]\n",
    "        # 辞書型で行動を紐づける\n",
    "        self.action_meaning = {\n",
    "            0: \"UP\",\n",
    "            1: \"DOWN\",\n",
    "            2: \"LEFT\",\n",
    "            3: \"RIGHT\"\n",
    "        }\n",
    "        # 以下にマップを作成する\n",
    "        # それぞれの値はその場所の報酬を指す\n",
    "        self.reward_map = np.array(\n",
    "            [[0,0,0,1],\n",
    "            [0,None,0,-1],\n",
    "            [0,0,0,0]]\n",
    "        )\n",
    "        #以下に障害物を記載し、その座標を保持\n",
    "        self.goal = (0,3)\n",
    "        self.start = (2,0)\n",
    "        self.wall = (1,1)\n",
    "        #エージェントの場所の配列を保持(スタート値で初期化)\n",
    "        self.agent_state = self.start\n",
    "\n",
    "    # 以下にメソッドを実装\n",
    "    # マップの情報を取得(主に再代入不可能のgetterの役割)\n",
    "    # 高さを返却\n",
    "    @property\n",
    "    def height(self):\n",
    "        return len(self.reward_map)\n",
    "    # 横幅を返却\n",
    "    @property\n",
    "    def width(self):\n",
    "        return len(self.reward_map[0])\n",
    "\n",
    "    #行動の種類を返却\n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self.action_list\n",
    "\n",
    "    def states(self):\n",
    "        for h in range(self.height):\n",
    "            for w in range(self.width):\n",
    "                yield (h,w)\n",
    "\n",
    "\n",
    "    # 以下にエージェントを動かすための関数と報酬関数を実装する。\n",
    "    def next_step(self,state,action):\n",
    "        # 上下左右の行動をそれぞれ座標に足し合わせることで実装\n",
    "        action_list = [(-1,0),(1,0),(0,-1),(0,1)]\n",
    "        # 0~3までの数字を配列に入れることで行動となる数値を取得\n",
    "        move = action_list[action]\n",
    "        # エージェントを移動させる(moveを現在の状態の座標に足し合わせる)\n",
    "        # next_state = (y座標,x座標)となる\n",
    "        next_state = (state[0] + move[0],state[1] + move[1])\n",
    "        new_y,new_x = next_state\n",
    "\n",
    "        # 移動したのちそれがマップから外れてるかどうかの確認処理(外れていたら更新を中断)\n",
    "        if new_x < 0 or new_x >= self.width or new_y < 0 or new_y >= self.height or next_state == self.wall:\n",
    "            next_state = state\n",
    "        return next_state #(2,3)みたいに次の座標\n",
    "    def reset(self):\n",
    "        self.agent_state = self.start\n",
    "        return self.agent_state\n",
    "    # 報酬関数\n",
    "    def reward(self,state,action,next_state):\n",
    "        return self.reward_map[next_state] # 1みたいに移動先での報酬を受け取る\n",
    "\n",
    "    # 新しいコード\n",
    "    # どう言った行動を起こしてどう言った報酬を受け取ったかを確認する。\n",
    "    # これで行動を起こした後にどの場所に行ったかを確認できる\n",
    "    def step(self, action):\n",
    "        state = self.agent_state\n",
    "        next_state = self.next_step(state, action)\n",
    "        reward = self.reward(state, action, next_state)\n",
    "        done = (next_state == self.goal)\n",
    "        self.agent_state = next_state\n",
    "        return next_state, reward, done\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 新しいモンテカルロ法用のクラスを実装\n",
    "class Monte():\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.9 # 割引率\n",
    "        self.action_size = 4 # 行動の数\n",
    "\n",
    "        random_actions = {0:0.25,1:0.25,2:0.25,3:0.25}\n",
    "        self.pi = defaultdict(lambda :random_actions)\n",
    "        self.V = defaultdict(lambda :0)\n",
    "        self.cnts = defaultdict(lambda :0)\n",
    "        self.memory = []\n",
    "    def get_action(self,state):\n",
    "        action_probs = self.pi[state]\n",
    "        actions = list(action_probs.keys())\n",
    "        probs = list(action_probs.values())\n",
    "        return np.random.choice(actions,p=probs) # pで確率を指定できるため,ここに指定しなければ(わからない場合は)\n",
    "\n",
    "    def add(self,state,action,reward):\n",
    "        data = (state,action,reward)\n",
    "        self.memory.append(data)\n",
    "    def reset(self):\n",
    "        self.memory.clear()\n",
    "    def eval(self):\n",
    "        G = 0\n",
    "        for data in reversed(self.memory):\n",
    "            state,action,reward = data\n",
    "            G =  self.gamma * G + reward\n",
    "            self.cnts[state] += 1\n",
    "            self.V[state] += (G - self.V[state])/ self.cnts[state]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function Monte.__init__.<locals>.<lambda> at 0x7ff240750b80>, {(1, 3): -0.37058160053012024, (1, 2): -0.5135380171882288, (0, 2): 0.18560701953442452, (0, 1): 0.08349730577296957, (0, 0): 0.015674548387005802, (1, 0): -0.02437761578173401, (2, 0): -0.10013788788646084, (2, 1): -0.21733898893290396, (2, 2): -0.4276820538360163, (2, 3): -0.7438097666642897})\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = Monte()\n",
    "episodes = 1000 # モンテカルロの試行回数\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state,reward,done = env.step(action)\n",
    "        agent.add(state,action,reward)\n",
    "        if done:\n",
    "            agent.eval()\n",
    "            break\n",
    "        state = next_state\n",
    "print(agent.V)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01567455  0.08349731  0.18560702  0.        ]\n",
      " [-0.02437762  0.         -0.51353802 -0.3705816 ]\n",
      " [-0.10013789 -0.21733899 -0.42768205 -0.74380977]]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((env.width, env.height))\n",
    "for i,j in agent.V.items():\n",
    "    a,b = i\n",
    "    X[a][b] = j\n",
    "print(np.array(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 1, 2, 3]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_actions = {0:0.25,1:0.25,2:0.25,3:0.25}\n",
    "pi = defaultdict(lambda :random_actions)\n",
    "state = (2,3)\n",
    "action_probs = pi[state]\n",
    "print(action_probs)\n",
    "actions = list(action_probs.keys())\n",
    "actions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}